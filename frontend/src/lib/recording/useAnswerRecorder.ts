import { useCallback, useEffect, useRef, useState } from 'react';
import { useInterviewAnswerStore } from '@/store/interviewAnswerStore';
import { type QuestionKey, type AnswerItem } from '@/types/interview';
import { sendFollowupAudio } from '@/api/interviewApi';
import { sendVideoUpload } from "@/api/interviewApi";

const extFromMime = (mt: string) =>
  mt.includes('webm') ? 'webm' : mt.includes('ogg') ? 'ogg' : 'wav';
interface ImageCapture {
  track: MediaStreamTrack;
  grabFrame(): Promise<ImageBitmap>;
  takePhoto?(photoSettings?: any): Promise<Blob>;
}
declare var ImageCapture: {
  prototype: ImageCapture;
  new (videoTrack: MediaStreamTrack): ImageCapture;
};
export function useAnswerRecorder({ 
  key, 
  maxDurationSec = 60,
  onUploadComplete,
  onInterviewFinished
}: { 
  key: QuestionKey; 
  maxDurationSec?: number;
  onUploadComplete?: () => void;
  onInterviewFinished?: () => void;
}) {
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<number | null>(null);
  const abortRef = useRef<AbortController | null>(null);
  const [isRecording, setIsRecording] = useState(false);
  const [seconds, setSeconds] = useState(0);
  const [error, setError] = useState<string | null>(null);

  // ë¡œì»¬ ë¯¸ë¦¬ë“£ê¸°/ìƒíƒœí‘œì‹œ ìš©(ì›ì¹˜ ì•Šìœ¼ë©´ ìŠ¤í† ì–´ ê´€ë ¨ ì „ë¶€ ì œê±° ê°€ëŠ¥)
  const setLocalPending = useInterviewAnswerStore((s) => s.setLocalPending);
  const markSynced = useInterviewAnswerStore((s) => s.markSynced);
  const markFailed = useInterviewAnswerStore((s) => s.markFailed);

  // ë¹„ë””ì˜¤ ì¶”ê°€
  const videoRecorderRef = useRef<MediaRecorder | null>(null);
  const videoChunksRef = useRef<Blob[]>([]);
  const [videoStream, setVideoStream] = useState<MediaStream | null>(null);
  // ìº”ë²„ìŠ¤ ë³´ë‚´ê¸°
  const processedStreamRef = useRef<MediaStream | null>(null);    // ìº”ë²„ìŠ¤ 30fps ìŠ¤íŠ¸ë¦¼
  const drawTimerRef = useRef<number | null>(null);
  // ì¸ë„¤ì¼ ì¶”ê°€
  const thumbBlobRef = useRef<Blob | null>(null);

  // ì—…ë¡œë“œ ì™„ë£Œ ìƒíƒœ ì¶”ì 
  const [audioUploaded, setAudioUploaded] = useState(false);
  const [videoUploaded, setVideoUploaded] = useState(false);

  // ì§ˆë¬¸ì´ ë°”ë€” ë•Œë§ˆë‹¤ ë…¹ìŒ ìƒíƒœ ë¦¬ì…‹
  useEffect(() => {
    // ê¸°ì¡´ íƒ€ì´ë¨¸ ì •ë¦¬
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    
    // ìƒíƒœ ì´ˆê¸°í™”
    setIsRecording(false);
    setSeconds(0);
    setError(null);
    setAudioUploaded(false);
    setVideoUploaded(false);
    
    // ê¸°ì¡´ ë ˆì½”ë” ì •ë¦¬
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    if (videoRecorderRef.current?.state === 'recording') {
      videoRecorderRef.current.stop();
    }
    
    // ref ì´ˆê¸°í™”
    mediaRecorderRef.current = null;
    videoRecorderRef.current = null;
    chunksRef.current = [];
    videoChunksRef.current = [];
    
    // ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì€ ìœ ì§€ (ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë™ì˜í•œ í™˜ê²½)
  }, [key.sessionId, key.order, key.subOrder]);

  // ì¸ë„¤ì¼ í•¨ìˆ˜
  const captureThumbFromStream = async (stream: MediaStream): Promise<Blob | null> => {
  const track = stream.getVideoTracks()[0];
  if (!track) return null;

  // ImageCapture ì§€ì› ì‹œ ìš°ì„  ì‚¬ìš©
  try {
    const win = window as any;
    if (win.ImageCapture && typeof win.ImageCapture === 'function') {
      const cap = new ImageCapture(track);
      const bitmap: ImageBitmap = await cap.grabFrame();

      const s = track.getSettings();
      const w = (s.width as number) ?? bitmap.width;
      const h = (s.height as number) ?? bitmap.height;

      const canvas = document.createElement('canvas');
      canvas.width = w;
      canvas.height = h;
      canvas.getContext('2d')!.drawImage(bitmap, 0, 0, w, h);

      return await new Promise<Blob | null>(res =>
        canvas.toBlob(b => res(b), 'image/jpeg', 0.9)
      );
    }
  } catch {}

  // B. í´ë°±: <video>ì— ìŠ¤íŠ¸ë¦¼ ë°”ì¸ë”© í›„ ì›ë³¸ í¬ê¸°ë¡œ ìº¡ì²˜
  return await new Promise<Blob | null>((resolve, reject) => {
    const v = document.createElement('video');
    v.muted = true;
    v.playsInline = true;
    // @ts-ignore
    v.srcObject = stream;

    v.onloadedmetadata = async () => {
      try {
        await v.play().catch(() => {});
        const w = v.videoWidth, h = v.videoHeight;
        if (!w || !h) return reject(new Error('ì˜ìƒ í¬ê¸° í™•ì¸ ì‹¤íŒ¨'));

        const c = document.createElement('canvas');
        c.width = w; c.height = h;
        c.getContext('2d')!.drawImage(v, 0, 0, w, h);
        c.toBlob(b => resolve(b), 'image/jpeg', 0.9);
      } catch (err) { reject(err as Error); }
    };
    v.onerror = () => reject(new Error('video load error'));
  });
};



  const stop = useCallback(() => {
    if (!mediaRecorderRef.current) return;
    if (timerRef.current) clearInterval(timerRef.current);
    setIsRecording(false);
    // ì”ì—¬ ë²„í¼ ì¦‰ì‹œ í”ŒëŸ¬ì‹œ ì‹œë„ í›„ ì •ì§€
    try { mediaRecorderRef.current.requestData?.(); } catch {}
    try { videoRecorderRef.current?.requestData?.(); } catch {}
    mediaRecorderRef.current.stop();
    // ë¹„ë””ì˜¤ë„ ë©ˆì¶¤
    if (videoRecorderRef.current?.state !== 'inactive') {
      videoRecorderRef.current?.stop();
    }
  }, []);

  const start = useCallback(async () => {
    setError(null);
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: true,
      video: {
        width: { ideal: 960, max: 960 },
        height: { ideal: 540, max: 540 },
        frameRate: { ideal: 30, max: 30 },
      }
      });
      // ì¸ë„¤ì¼
      const vtrack = stream.getVideoTracks()[0];
      try {
        await vtrack.applyConstraints({
          width: { exact: 960 },
          height: { exact: 540 },
          frameRate: { exact: 30 },
        });
      } catch { /* ë¯¸ì§€ì›ì´ë©´ í˜‘ìƒëœ ê°’ ì‚¬ìš© */ }


    // ë¹„ë””ì˜¤ ê°’
    setVideoStream(stream);

   // ì‹œì‘ ì‹œì  ì¸ë„¤ì¼ ìº¡ì³
    try {
      thumbBlobRef.current = await captureThumbFromStream(stream);
    } catch {
      thumbBlobRef.current = null;
    }

    
    // ìº”ë²„ìŠ¤ ê·¸ë¦¬ê¸° 30fps
    const w = 960, h = 540;
    const canvas = document.createElement('canvas'); canvas.width = w; canvas.height = h;
    const ctx = canvas.getContext('2d')!;
    const vid = document.createElement('video'); vid.srcObject = stream; vid.muted = true; await vid.play();

    const draw = () => {
      ctx.save();
      ctx.translate(w, 0); ctx.scale(-1, 1); // â† ë¯¸ëŸ¬ í•„ìš”ì‹œ í™œì„±í™”
      ctx.drawImage(vid, 0, 0, w, h);
      ctx.restore();
    };
    drawTimerRef.current = window.setInterval(draw, 1000/30); // **30fps ê³ ì •**
    const processed = canvas.captureStream(30);                // **30fps ìŠ¤íŠ¸ë¦¼**
    const aTrack = stream.getAudioTracks()[0];
    if (aTrack) processed.addTrack(aTrack);
    processedStreamRef.current = processed;

    // ìº”ë²„ìŠ¤ ë

    // ì˜¤ë””ì˜¤ ì „ìš©
    const audioOnly = new MediaStream(stream.getAudioTracks());
    const mr = new MediaRecorder(audioOnly);
    chunksRef.current = [];

    mr.ondataavailable = (e) => { if (e.data.size) chunksRef.current.push(e.data); };

    mr.onstop = async () => {
      const blob = new Blob(chunksRef.current, { type: mr.mimeType });
      
      // webm -> wav ë³€í™˜
      const arrayBuf = await blob.arrayBuffer();
      const AudioCtx = (window.AudioContext || (window as any).webkitAudioContext);
      const audioCtx = new AudioCtx()
      
      const decoded: AudioBuffer = await new Promise((resolve, reject) =>
        audioCtx.decodeAudioData(arrayBuf.slice(0), resolve, reject)
      );

    function audioBufferToWav(buffer: AudioBuffer): ArrayBuffer {
        const numChannels = buffer.numberOfChannels;
        const sampleRate = buffer.sampleRate;
        const bitsPerSample = 16;
        const samples = buffer.length;

        const blockAlign = (numChannels * bitsPerSample) >> 3;
        const byteRate = sampleRate * blockAlign;
        const dataSize = samples * blockAlign;

        const ab = new ArrayBuffer(44 + dataSize);
        const view = new DataView(ab);

        const writeString = (off: number, s: string) => {
          for (let i = 0; i < s.length; i++) view.setUint8(off + i, s.charCodeAt(i));
        };

        // RIFF/WAVE í—¤ë”
        writeString(0, "RIFF");
        view.setUint32(4, 36 + dataSize, true);
        writeString(8, "WAVE");
        writeString(12, "fmt ");
        view.setUint32(16, 16, true); // PCM
        view.setUint16(20, 1, true);  // PCM format
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, byteRate, true);
        view.setUint16(32, blockAlign, true);
        view.setUint16(34, bitsPerSample, true);
        writeString(36, "data");
        view.setUint32(40, dataSize, true);

        // PCM interleave (float32 -> int16)
        const channels: Float32Array[] = [];
        for (let ch = 0; ch < numChannels; ch++) channels.push(buffer.getChannelData(ch));
        let offset = 44;
        for (let i = 0; i < samples; i++) {
          for (let ch = 0; ch < numChannels; ch++) {
            const x = Math.max(-1, Math.min(1, channels[ch][i]));
            view.setInt16(offset, x < 0 ? x * 0x8000 : x * 0x7fff, true);
            offset += 2;
          }
        }
        return ab;
      }

      const wavAB = audioBufferToWav(decoded);
      audioCtx.close?.();
      const wavBlob = new Blob([wavAB], { type: "audio/wav" });

      // webm -> wav ë³€í™˜ ë


      const localUrl = URL.createObjectURL(wavBlob);
      const now = new Date().toISOString();

      // ì €ì¥ì€ ì•ˆ í•˜ì§€ë§Œ, UI ë¯¸ë¦¬ë“£ê¸°ì™€ ìƒíƒœí‘œì‹œë¥¼ ìœ„í•´ pendingë§Œ ê¸°ë¡
      setLocalPending({
        key, localBlobUrl: localUrl, durationSec: seconds,
        mimeType: "audio/wav", createdAt: now, syncStatus: 'pending',
      } as AnswerItem);

      try {
        abortRef.current = new AbortController();
        const order = localStorage.getItem("currentOrder");
        const subOrder = localStorage.getItem("currentSubOrder");
        const sessionId = localStorage.getItem("interviewSessionId");
        const fileName = `answer_${sessionId}_o${order}_s${subOrder}_${Date.now()}.wav`;
        const file = new File([wavBlob], fileName, { type: "audio/wav" });
        console.log("audiofile=======",file);
        const result = await sendFollowupAudio({
          sessionId: key.sessionId,
          order1: key.order,
          subOrder: key.subOrder,
          audio: file,
        });

        // ë©´ì ‘ ì™„ë£Œ ì²´í¬
        if (result?.finished) {
          console.log("ğŸ‰ useAnswerRecorder: ë©´ì ‘ ì™„ë£Œ ê°ì§€!");
          onInterviewFinished?.();
          return; // ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
        }

        // ì‘ë‹µ ë°”ë””ê°€ ì—†ë‹¤ â†’ ì„±ê³µë§Œ í‘œê¸°
        markSynced(key, {});
        setAudioUploaded(true);
      } catch (e: any) {
        setError(e?.message ?? 'upload failed');
        markFailed(key, e?.message ?? 'upload failed');
      } finally {
        abortRef.current = null;
      }
    }

      // ë¹„ë””ì˜¤ ì €ì¥
      // let videoMR = new MediaRecorder(stream);
    let webmOptions: MediaRecorderOptions | undefined;
    const tryMime = (mt: string) => (window as any).MediaRecorder?.isTypeSupported?.(mt);
    if (tryMime?.('video/webm;codecs=vp8')) webmOptions = { mimeType: 'video/webm;codecs=vp8', videoBitsPerSecond: 1_500_000, audioBitsPerSecond: 128_000 };
    else if (tryMime?.('video/webm')) webmOptions = { mimeType: 'video/webm', videoBitsPerSecond: 1_500_000, audioBitsPerSecond: 128_000 };
    else webmOptions = { videoBitsPerSecond: 1_500_000, audioBitsPerSecond: 128_000 };

      let videoMR = new MediaRecorder(processed, webmOptions);

      videoRecorderRef.current = videoMR;
      videoChunksRef.current = [];
      videoMR.ondataavailable = (e) => { if (e.data.size) videoChunksRef.current.push(e.data); };

      videoMR.onstop = async () => {
          // í˜„ì¬ video track fps í™•ì¸
      const vTrack = processedStreamRef.current?.getVideoTracks()[0];
      if (vTrack) {
        const settings = vTrack.getSettings();
        console.log("[ë…¹í™” FPS]", settings.frameRate ?? "ì•Œ ìˆ˜ ì—†ìŒ");
      }
        const usedMime = videoMR.mimeType || 'video/webm';
          const vblob = new Blob(videoChunksRef.current, { type: usedMime});
          if(vblob.size===0){
            console.warn('ë…¹í™” ë¹„ë””ì˜¤ ì—†ìŒ')
            return;
          }
          const order = localStorage.getItem("currentOrder");
          const subOrder = localStorage.getItem("currentSubOrder");
        
          const file = new File([vblob],
            `${order}_${subOrder}.webm`,
            { type: usedMime }
          )
          const calibData = localStorage.getItem("gaze_calibration_data");
          const formData = new FormData();
          formData.append("file", file);
          formData.append("interviewSessionId", localStorage.getItem("interviewSessionId") ?? "");
          formData.append("order", order ?? "0" );
          formData.append("subOrder", subOrder ?? "0");
          formData.append("calibDataJson", JSON.stringify(calibData));


          const thumb = thumbBlobRef.current;
          if (thumb) {
            const thumbFile = new File(
              [thumb],
              `${order}_${subOrder}.jpg`,
              { type: "image/jpeg" }
            );
            formData.append("thumbnail", thumbFile)
          }

          // ë™ì˜ìƒ ì „ì†¡ í›„ url return
          const urls = await sendVideoUpload(formData);
          console.log("==========ë¹„ë””ì˜¤ ë ˆì¸ ê³  ======", urls);


          // ì¹´ë©”ë¼ë¥¼ í•­ìƒ ìœ ì§€: íŠ¸ë™/ìŠ¤íŠ¸ë¦¼ì€ ìœ ì§€í•˜ê³  ë ˆì½”ë”ë§Œ ì •ë¦¬
          videoRecorderRef.current = null;

          if (drawTimerRef.current) { clearInterval(drawTimerRef.current); drawTimerRef.current = null; }
          processedStreamRef.current = null;

        };
    
    mr.start(100);
    //ë¹„ë””ì˜¤
    videoMR.start(100);
    mediaRecorderRef.current = mr;
    setIsRecording(true);
    setSeconds(0);

    timerRef.current = window.setInterval(() => {
      setSeconds((s) => { const n = s + 1; if (n >= maxDurationSec) stop(); return n; });
    }, 1000);
  }, [maxDurationSec, setLocalPending, markSynced, markFailed, stop]);

  useEffect(() => {
    if (onUploadComplete && audioUploaded && videoUploaded) {
      onUploadComplete();
    }
  }, [audioUploaded, videoUploaded, onUploadComplete]);

  useEffect(() => () => {
    if (timerRef.current) clearInterval(timerRef.current);
    if (abortRef.current) abortRef.current.abort();
    if (mediaRecorderRef.current?.state === 'recording') mediaRecorderRef.current.stop();
    if (videoRecorderRef.current?.state === 'recording') videoRecorderRef.current.stop();
    if (drawTimerRef.current) { clearInterval(drawTimerRef.current); drawTimerRef.current = null; }
  }, []);

  return { start, stop, isRecording, seconds, error, videoStream };
}
